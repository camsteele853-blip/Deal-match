import { callMCPTool } from '@/sdk/core/mcp-client';

/**
 * MCP Response wrapper interface - MANDATORY
 * All MCP tools return responses in this wrapped format
 */
interface MCPToolResponse {
  content: Array<{
    type: "text";
    text: string; // JSON string containing actual tool data
  }>;
}

/**
 * Input parameters for Perplexity AI search completion request
 */
export interface PerplexitySearchParams {
  /**
   * Multiplicative penalty for new tokens based on their frequency in the text to avoid repetition.
   * Mutually exclusive with the 'presence_penalty' parameter.
   * @example 0.5, 1.0, 1.5
   */
  frequency_penalty?: number;

  /**
   * The maximum number of tokens to generate. Sum of max_tokens and prompt tokens should not exceed
   * the model's context window limit. Unspecified leads to generation until stop token or context window end.
   * @example 100, 150, 200
   */
  max_tokens?: number;

  /**
   * The name of the model to use for generating completions.
   * Choose a model based on the desired balance between performance and resource usage.
   * For more information check https://docs.perplexity.ai/guides/model-cards
   * @default "sonar"
   */
  model?: "sonar" | "sonar-reasoning-pro" | "sonar-reasoning" | "sonar-pro";

  /**
   * Penalty for new tokens based on their current presence in the text, encouraging topic variety.
   * Mutually exclusive with the 'frequency_penalty' parameter.
   * Range: -2.0 to 2.0
   * @example -2.0, 0.0, 2.0
   */
  presence_penalty?: number;

  /**
   * Whether to include citations in the model's response.
   * Citations feature is in closed beta.
   * @example true, false
   */
  return_citations?: boolean;

  /**
   * Whether to include images in the model's response.
   * Image generation feature is in closed beta.
   * @example true, false
   */
  return_images?: boolean;

  /**
   * Whether to stream the response incrementally using server-sent events.
   * @example true, false
   */
  stream?: boolean;

  /**
   * The system's content for specifying instructions to guide the model's behavior.
   * @default "You are a helpful assistant that provides accurate and informative responses."
   * @example "Be precise and concise.", "You are a helpful assistant."
   */
  systemContent?: string;

  /**
   * Controls generation randomness, with 0 being deterministic and values approaching 2 being more random.
   * Range: 0 to <2
   * @example 0.0, 0.7, 1.5
   */
  temperature?: number;

  /**
   * Limits the number of high-probability tokens to consider for generation. Set to 0 to disable.
   * Range: 0 to 2048
   * @example 0, 40, 80
   */
  top_k?: number;

  /**
   * Nucleus sampling threshold, controlling the token selection pool based on cumulative probability.
   * Range: 0 to 1
   * @example 0.1, 0.9, 1.0
   */
  top_p?: number;

  /**
   * The user's Content for asking questions or providing input.
   * @example "How many stars are there in our galaxy?"
   */
  userContent: string;
}

/**
 * Message generated by the model
 */
export interface PerplexityMessage {
  /**
   * Content of the message
   */
  content: string;

  /**
   * Role of the message sender (system, user, or assistant)
   */
  role: string;
}

/**
 * Completion choice generated by the model
 */
export interface PerplexityChoice {
  /**
   * Reason why the model stopped generating (e.g., 'stop')
   */
  finish_reason: string;

  /**
   * Index of this choice in the list of choices
   */
  index: number;

  /**
   * The message generated by the model
   */
  message: PerplexityMessage;
}

/**
 * Search result used to ground the response
 */
export interface PerplexitySearchResult {
  /**
   * Date of the search result (YYYY-MM-DD format)
   */
  date?: string;

  /**
   * Source type of the search result (e.g., 'web')
   */
  source?: string;

  /**
   * Title of the search result
   */
  title: string;

  /**
   * URL of the search result
   */
  url?: string;
}

/**
 * Token usage information for this completion
 */
export interface PerplexityUsage {
  /**
   * Number of tokens used for citations
   */
  citation_tokens?: number;

  /**
   * Number of tokens in the completion
   */
  completion_tokens: number;

  /**
   * Number of search queries performed
   */
  num_search_queries?: number;

  /**
   * Number of tokens in the prompt
   */
  prompt_tokens: number;

  /**
   * Number of tokens used for reasoning
   */
  reasoning_tokens?: number;

  /**
   * Size of the search context
   */
  search_context_size?: string;

  /**
   * Total number of tokens used
   */
  total_tokens: number;
}

/**
 * Video related to the query
 */
export interface PerplexityVideo {
  /**
   * Duration of the video in seconds
   */
  duration: number;

  /**
   * Height of the video thumbnail in pixels
   */
  thumbnail_height: number;

  /**
   * URL of the video thumbnail
   */
  thumbnail_url: string;

  /**
   * Width of the video thumbnail in pixels
   */
  thumbnail_width: number;

  /**
   * URL of the video
   */
  url: string;
}

/**
 * Output data from Perplexity AI search completion
 */
export interface PerplexitySearchData {
  /**
   * List of completion choices generated by the model
   */
  choices: PerplexityChoice[];

  /**
   * Unix timestamp (in seconds) when the completion was created
   */
  created: number;

  /**
   * Unique identifier for the completion
   */
  id: string;

  /**
   * Model used for generating the completion
   */
  model: string;

  /**
   * Object type, always 'chat.completion'
   */
  object: string;

  /**
   * Search results used to ground the response (if available)
   */
  search_results?: PerplexitySearchResult[];

  /**
   * Token usage information for this completion
   */
  usage: PerplexityUsage;

  /**
   * Videos related to the query (if available)
   */
  videos?: PerplexityVideo[];
}

/**
 * Internal response wrapper interface from outputSchema
 */
interface PerplexitySearchResponse {
  /**
   * Whether or not the action execution was successful or not
   */
  successful: boolean;

  /**
   * Data from the action execution
   */
  data?: PerplexitySearchData;

  /**
   * Error if any occurred during the execution of the action
   */
  error?: string;
}

/**
 * Performs a Perplexity AI search with the specified parameters and returns completion results.
 * This tool uses Perplexity's AI models to generate responses based on user queries,
 * with optional search grounding, citations, and images.
 *
 * @param params - The input parameters for the Perplexity AI search
 * @returns Promise resolving to the completion data including choices, usage, and optional search results
 * @throws Error if required parameter 'userContent' is missing or if the tool execution fails
 *
 * @example
 * const result = await request({
 *   userContent: 'How many stars are there in our galaxy?',
 *   model: 'sonar',
 *   temperature: 0.7
 * });
 */
export async function request(params: PerplexitySearchParams): Promise<PerplexitySearchData> {
  // Validate required parameters
  if (!params.userContent) {
    throw new Error('Missing required parameter: userContent');
  }

  // Validate mutually exclusive parameters
  if (params.frequency_penalty !== undefined && params.presence_penalty !== undefined) {
    throw new Error('Parameters frequency_penalty and presence_penalty are mutually exclusive');
  }

  // Validate parameter ranges
  if (params.frequency_penalty !== undefined && params.frequency_penalty <= 0) {
    throw new Error('Parameter frequency_penalty must be greater than 0');
  }

  if (params.presence_penalty !== undefined && (params.presence_penalty < -2 || params.presence_penalty > 2)) {
    throw new Error('Parameter presence_penalty must be between -2 and 2');
  }

  if (params.temperature !== undefined && (params.temperature < 0 || params.temperature >= 2)) {
    throw new Error('Parameter temperature must be between 0 and <2');
  }

  if (params.top_k !== undefined && (params.top_k < 0 || params.top_k > 2048)) {
    throw new Error('Parameter top_k must be between 0 and 2048');
  }

  if (params.top_p !== undefined && (params.top_p < 0 || params.top_p > 1)) {
    throw new Error('Parameter top_p must be between 0 and 1');
  }

  // CRITICAL: Use MCPToolResponse and parse JSON response
  const mcpResponse = await callMCPTool<MCPToolResponse, PerplexitySearchParams>(
    '6875e6198345ff1a8579cd8a',
    'PERPLEXITYAI_PERPLEXITY_AI_SEARCH',
    params
  );

  if (!mcpResponse.content?.[0]?.text) {
    throw new Error('Invalid MCP response format: missing content[0].text');
  }

  let toolData: PerplexitySearchResponse;
  try {
    toolData = JSON.parse(mcpResponse.content[0].text);
  } catch (parseError) {
    throw new Error(
      `Failed to parse MCP response JSON: ${
        parseError instanceof Error ? parseError.message : 'Unknown error'
      }`
    );
  }

  if (!toolData.successful) {
    throw new Error(toolData.error || 'MCP tool execution failed');
  }

  if (!toolData.data) {
    throw new Error('MCP tool returned successful response but no data');
  }

  return toolData.data;
}